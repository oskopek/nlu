{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "\n",
    "if not os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"notebooks\")\n",
    "assert os.getcwd().endswith(\"notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"..\", \"data\")\n",
    "train_file = os.path.join(data_dir, \"stories.train.csv\")\n",
    "eval_file = os.path.join(data_dir, \"stories.eval.csv\")\n",
    "# test_file = \"\"\n",
    "\n",
    "UNK = \"<unk>\"\n",
    "PAD = \"<pad>\"\n",
    "\n",
    "nonalphanumeric_pattern = re.compile('([^0-9a-zA-Z ])')\n",
    "\n",
    "def uncommon_filter(df):\n",
    "    vocab = {}\n",
    "    for i, row in df.iterrows():\n",
    "        row_vocab = set()\n",
    "        for col in row.keys():\n",
    "            if not col.startswith(\"sentence\") and not col.startswith(\"ending\"):\n",
    "                continue\n",
    "            sentence = row[col]\n",
    "            assert isinstance(sentence, str)\n",
    "            tokens = sentence.split()\n",
    "            row_vocab.update(set(tokens))\n",
    "        for word in row_vocab:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 0\n",
    "    removed = []\n",
    "    for word in vocab.keys():\n",
    "        if vocab[word] <= 1:\n",
    "            removed.append(word)\n",
    "    print(len(removed))\n",
    "#     for i, row in df.iterrows():\n",
    "#         ifor_val = something\n",
    "#         if <condition>:\n",
    "#             ifor_val = something_else\n",
    "#         df.set_value(i,'ifor',ifor_val)\n",
    "    return df\n",
    "\n",
    "def skip_ner(tokens_cased):\n",
    "    pos_tags = nltk.pos_tag(tokens_cased)\n",
    "    ne_chunks = nltk.ne_chunk(pos_tags, binary=True)\n",
    "    idx = 0\n",
    "    ne_lst = []\n",
    "    for ne in ne_chunks:\n",
    "        if isinstance(ne, nltk.tree.Tree):\n",
    "            for leaf in ne.leaves():\n",
    "                ne_lst.append(idx)\n",
    "                idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "\n",
    "    res = []\n",
    "    for i, token in enumerate(tokens_cased):\n",
    "        if i in ne_lst:\n",
    "#             res.append(\"NE_\" + str(token)) # TODO: Mark them for attention\n",
    "            # TODO: Consider finidng all occurrences of token that was marked as NE in surrounding sentences and marking it as NE as well, if capitalization matches\n",
    "            res.append(UNK)\n",
    "        else:\n",
    "            res.append(token)\n",
    "    return res\n",
    "\n",
    "def normalize_sentence(sentence, NER=False, uncommon_words=False):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    if NER:\n",
    "        tokens = skip_ner(tokens)\n",
    "        \n",
    "    tokens_lo = [token.lower() for token in tokens]\n",
    "    sentence = \" \".join(tokens_lo) # normalize spaces\n",
    "    return sentence\n",
    "\n",
    "FLAG_NER = False\n",
    "FLAG_UNCOMMON = True\n",
    "\n",
    "def read_train(file):\n",
    "    def _read(file):\n",
    "        df = pd.read_csv(file)\n",
    "        df = df[:200] # TODO: REMOVE ME\n",
    "        del df['storytitle']\n",
    "        df = df.rename(index=str, columns={\"sentence5\": \"ending\"})\n",
    "        return df\n",
    "    \n",
    "    # storyid, sentence1, sentence2, sentence3, sentence4, ending\n",
    "    char_df = _read(file)\n",
    "    word_df = _read(file)\n",
    "    sentence_selector = ['sentence1', 'sentence2', 'sentence3', 'sentence4', 'ending']\n",
    "    word_df[sentence_selector] = word_df[sentence_selector].applymap(lambda sent: normalize_sentence(sent, NER=FLAG_NER))\n",
    "    if FLAG_UNCOMMON:\n",
    "        word_df = uncommon_filter(word_df)\n",
    "    \n",
    "    return char_df, word_df\n",
    "\n",
    "def read_eval(file):\n",
    "    def _read(file):\n",
    "        df = pd.read_csv(file)\n",
    "        df = df[:200] # TODO: REMOVE ME\n",
    "        df = df.rename(index=str, columns={\"InputStoryid\": \"storyid\", \"InputSentence1\": \"sentence1\", \"InputSentence2\": \"sentence2\", \"InputSentence3\": \"sentence3\", \"InputSentence4\": \"sentence4\", \"RandomFifthSentenceQuiz1\": \"ending1\", \"RandomFifthSentenceQuiz2\": \"ending2\", \"AnswerRightEnding\": \"label\"})\n",
    "        return df\n",
    "    \n",
    "    # storyid, sentence1, sentence2, sentence3, sentence4, ending1, ending2, answer\n",
    "    char_df = _read(file)\n",
    "    word_df = _read(file)\n",
    "    sentence_selector = ['sentence1', 'sentence2', 'sentence3', 'sentence4', 'ending1', 'ending2']\n",
    "    word_df[sentence_selector] = word_df[sentence_selector].applymap(lambda sent: normalize_sentence(sent, NER=FLAG_NER))\n",
    "    if FLAG_UNCOMMON:\n",
    "        word_df = uncommon_filter(word_df)\n",
    "    \n",
    "    return char_df, word_df\n",
    "\n",
    "train_char_df, train_word_df = read_train(train_file)\n",
    "eval_char_df, eval_word_df = read_eval(eval_file)\n",
    "\n",
    "display(train_word_df.iloc[190])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(eval_word_df['sentence1'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_df.to_csv('../outputs/words_train.csv', index=False, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "train_char_df.to_csv('../outputs/chars_train.csv', index=False, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "eval_word_df.to_csv('../outputs/words_eval.csv', index=False, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "eval_char_df.to_csv('../outputs/chars_eval.csv', index=False, quotechar='\"', quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* How many uncommon words are in the last sentence?\n",
    "* Link NEs to label sentence, same for eval\n",
    "* Find words that only occur in <= C stories in dataset (C=1), and make them UNK?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_train(df):\n",
    "    d = {PAD: 0, UNK: 1}\n",
    "    uid = 2\n",
    "    for idx, story_id, title, *sentences in df.itertuples():\n",
    "        for sentence in sentences:\n",
    "            for word in sentence.strip().split(\" \"):\n",
    "                if word not in d:\n",
    "                    d[word] = uid\n",
    "                    uid += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_voc = vocab_train(train_word_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train vocab\", len(train_voc))\n",
    "print(train_voc[\"youtube\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "* CRLF line breaks\n",
    "* Empty line at end\n",
    "* answer.txt name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join(\"..\", \"outputs\")\n",
    "eval_out_file = os.path.join(output_folder, \"eval\", \"answer.txt\")\n",
    "test_out_file = os.path.join(output_folder, \"test\", \"answer.txt\")\n",
    "\n",
    "def output_official(results, out_file):\n",
    "    out_df = results[[\"InputStoryId\", \"AnswerRightEnding\"]]\n",
    "    pass\n",
    "\n",
    "def output_nlu(results, out_file):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
