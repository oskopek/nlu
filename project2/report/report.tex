\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{square,numbers,comma,sort}{natbib}
\bibliographystyle{plain}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% Choose a title for your submission
\title{NLU Task 2: Story Cloze Task --- Report}

\author{
  Luk\'{a}\v{s} Jendele$^\ast$\\
  ETH Zurich\\
  \texttt{jendelel@ethz.ch}\\
  %% examples of more authors
  \And
  Ondrej Skopek$^\ast$\\
  ETH Zurich\\
  \texttt{oskopek@ethz.ch}\\
  \And
  Vasily Vitchevsky$^\ast$\\
  ETH Zurich\\
  \texttt{vasilyv@ethz.ch}\\
  \And
  Micheal Wiegner\thanks{All authors contributed equally.}\\
  ETH Zurich\\
  \texttt{wiegnerm@ethz.ch}\\
}

\begin{document}
\maketitle

% We do not require you to write an abstract. Still, if you feel like it, please do so.
%\begin{abstract}
%In this work, we evaluate various neural network based discriminative approaches to the Story Cloze Task. We investigate several variations of the models in Roemmele\,et\,al.\,\citep{Roemmele2017AnTest}. Our approaches maintain the simplicity and natural approach to the task of the original work. The best models achieve a slightly higher accuracy, while the worse performing ones present interesting negative results.
%\end{abstract}

\section{Introduction}\label{sec:intro}
In the past few years, deep learning models have shown great potential in many areas regarding natural language understanding such as sentiment analysis, language modeling, or machine translation. Unfortunately, research on reading comprehension and logical induction is still in its very early stages. In our work, we attempt to tackle the Story Cloze Task \citep{Mostafazadeh2016AStories}. In this task, one tries to automatically choose the correct ending for a short story. Unlike in other tasks, the Story Cloze task's training and validation phases are conceptually different: during training, the model is given a story consisting of 5 short sentences, whereas for validation, the model is given the first four sentences of a story and two possibilities for the last sentence. The model has to then choose the correct one, i.\,e.\,the most logical ending for the story out of the two possible endings. The idea behind this task is to see how well the model can make use of the semantic links between the sentences of the short story and the ending which are necessary to choose the correct answer.

The most challenging part is the lack of negative endings during the training phase. Models trained on the validation dataset \citep{Srinivasan2018ATest,Roemmele2017AnTest} have shown much higher accuracy on test data than models trained exclusively on the actual training dataset \citep{Roemmele2017AnTest,Wang2017ConditionalComprehension}, although the training data is much larger. This problem is caused by the lack of a natural loss function choice for the two different phases. Researches have been solving this issue either by generating negative samples (using generative methods: GANs \citep{Wang2017ConditionalComprehension} or language modeling \citep{Roemmele2017AnTest}), or by sampling negative endings from the training dataset (discriminative methods \citep{Roemmele2017AnTest}). However, all methods achieve significantly worse results than when training on the validation data.

Another big difficulty is the large natural variability in plausible and implausible endings for a given story. Interestingly, humans are able to achieve an accuracy of 100\% for this task, indicating that the task is perfectly solvable despite all the pointed out difficulties.

In our project, we decided to sample the negative endings from the other endings in the training dataset. We reproduced the work by Roemmele\,et\,al.\,\citep{Roemmele2017AnTest} and managed to increase the accuracy. The improved accuracy is achieved using multiple approaches.

% \section{Failures}
% We implemented the following variations of Roemmele's work: vanilla RNN (with a basic RNN cell), temporal CNN for text sequences \citep{Kim2014ConvolutionalClassification}, Bahdanau \citep{BahdanauEND-TO-ENDRECOGNITION} and Luong \citep{Luong2015EffectiveTranslation} Attention, LSTM \citep{Hochreiter1997LONGMEMORY}. None of these methods achieved significantly better performance than the original Roemmele \citep{Roemmele2017AnTest} work. 

%Ideally you introduce the task already in a way that highlights the difficulties your method will tackle.

\section{Methodology \& Model}\label{sec:methodology}
Our method follows closely the method described in Roemmele\,et\,al.\,\citep{Roemmele2017AnTest}. We use a discriminative binary classifier conditioned on the context sentence. Given the first 4 sentences, the classifier assigns a probability of being a plausible ending to the last sentence. To overcome the lack of negative endings during training, we sample them randomly from other endings the training dataset. Our models utilize the BookCorpus \citep{Zhu2015AligningBooks} dataset by embedding the story and ending sentences using a pre-trained embedding model. Specifically, we use Skip-Thought sentence embeddings \citep{Kiros2015Skip-ThoughtVectors}.\footnote{We use the implementation in TensorFlow Models by Chris Shallue: \url{https://github.com/tensorflow/models/tree/master/research/skip_thoughts}} We concatenate the sentence embeddings from both uni-skip and bi-skip Skip-Thought models. Our experiments have shown that using both embedding models yields the best results (as already noted in the original paper). The binary classifier takes the Skip-Thought embeddings, and predicts the last sentence's plausibility (probability of it being a plausible ending). During validation, the model chooses the most probable ending of the two candidates.

\subsection{Re-implementation of RNN GRU}
We ran a static one-directional 1000-dimensional GRU RNN network on the five Skip-Thought embeddings. We use the final state as input to an dense output layer with one output neuron, with a sigmoid activation function -- which models the conditional probability that the last sentence is plausible.

\subsection{RNN with different cells}
After reimplementing the original work, we investigated the impact of replacing the original GRU cell with the more powerful LSTM cell and a simple basic RNN cell (dense layer and $\tanh$ activation function). Since Dropout \citep{Srivastava2014} has been shown to improve performance of models in many use cases, we tried to apply a Dropout layer on the inputs of the GRU cell, but the effects on performance were not significant.

\subsection{RNN shifted negative endings}
Inspired by a blog post\footnote{Ed Rushton and Jeremy P Howard. Any libraries/dictionaries out there for fixing common spelling errors? \textit{forums.fast.ai}, 2018. URL: \url{http://forums.fast.ai/t/nlp-any-libraries-dictionaries-out-there-for-fixing-common-spelling-errors/16411/8}.} claiming that misspellings in word embeddings can be calculated using the difference between a misspelled word and a correct word, we tried to generate sentence embeddings of the negative endings for the positive endings in the training dataset. 

Using the first 100 stories in the evaluation set, we calculated the average difference between the wrong endings and the correct endings. Then we trained our binary classifier with negative endings calculated as the sum of correct ending and the pre-calculated average difference.

We realize that in high-dimensional spaces, it is more common to use cosine similarity. Unfortunately, we did not come up with an idea of how to sample negative endings with given cosine distance from the correct ending.

\subsection{RNN with attention}
As a logical next step after training a simple RNN, we try adding an attention mechanism -- both multiplicative (Luong \citep{Luong2015EffectiveTranslation}) and additive (Bahdanau \citep{Bahdanau2016End-to-EndRecognition}) -- to the model. Given that the sequences are short, we do not expect a big increase in performance.

The mechanisms were designed for translation in a sequence to sequence fashion. For our use case, we have a a 5-step sequence on the ``encoder'' side and a ``single-step'' sequence on the ``decoder'' side (the final state from our RNN). We compute alignments and successively the averaged attention state, and concatenate it to the RNN's final state before passing both to the fully-connected output layer as previously mentioned.

\subsection{Temporal CNN}
Instead of an RNN, we ran several temporal convolutions with different kernel sizes (3,4,5) on these embeddings in parallel, similar to Kim \citep{Kim2014ConvolutionalClassification}. Lastly, we concatenate the CNN feature maps and use a 300 output sized hidden dense layer with Dropout. The motivation for Temporal CNN was a potential speed-up due to parallelization that is not possible with Gated Recurrent Units (GRUs), at the expense of a higher number of model parameters (30 million in the TCNN model compared to 17.4 million in the GRU model. Due to the number of parameters and the short length of sequences (5), the potential parallelization advantage was not fully leveraged, and the TCNN model was actually slower during training.



\section{Training}\label{sec:training}
All models are trained with a dense layer with one sigmoid-activated output at the end, and a cross-entropy loss function with labels 0 and 1. The label 1 means that the ending is plausible, and 0 means it is not. We sampled random negative endings from other stories with the same ratio 1:6 as in Roemmele\,et\,al. We use mini-batches of size 100 and clip gradients to a maximum $L_2$ norm of 10. In terms of optimization, we tried Adam \citep{Kingma2015}, AdaDelta \citep{Zeiler2012ADADELTA:Method}, RMSProp \citep{HintonNeuralDescent}, and standard SGD, all of them with learning rate $0.001$. Our experiments confirmed that RMSProp \citep{HintonNeuralDescent} achieves the highest scores for all models except for the Temporal CNN model, where Adam performed better.

We ran all experiment on GeForce GTX 1080 Ti on ETH Leonhard cluster. Data processing takes about 15 minutes and training a single epoch about 5 minutes. The difference between the different models are negligible.

\section{Experiments}\label{sec:experiments}
We trained all models on training data for 10 epochs and evaluated on the validation dataset after every 2000 steps.
The TCNN model was trained for 30 epochs.

During the evaluation, we keep the best 3 checkpoints of every model, according to validation set accuracy.
Finally, we measure the accuracy on the test dataset (ROC 2016), using the best of the three checkpoints for each model. 

The results are presented in Table \ref{tab:results}.
Interestingly, our implementation of Roemmele's model performs significantly better than the original implementation,
according to the reported accuracies. We believe this might be caused by the superior quality of the pre-trained Skip-Thought embeddings we used.

Generating sentence embeddings for negative endings from the embedding of the correct ones has proven to have potential,
however as due to the nature of the method we trained with labels in a 1:1 ratio instead of 1:6,
the model converged very quickly and started overfitting. We believe generating more negative samples or decreasing the model complexity would help.

As shown in Table \ref{tab:results}, LSTMs generalize better than GRUs (almost no difference between validation and test), which is probably caused by the extra gates in the cell.
A large-scale comparison of RNN cells confirms that out of a lot of cells, LSTMs achieve the best performance in NLU tasks\citep{JozefowiczAnArchitectures}.

The vanilla RNN is too simple to learn meaningful features from context and does not reach 60\% accuracy.

The bi-directional GRU falls behind in accuracy because the right context is likely not that important (backward pass).
Moreover, Srinivasan\,et\,al.\,\citep{Srinivasan2018ATest} show that the last sentence is the most important information for this task, which confirms our claim.

Both attention models perform just as well as the respective models without them. This points to the fact that the attention mechanisms are not useful here (short sequences), the RNN cells have enough capacity to capture the context on their own. We also tried running models without the RNN (just with the attention), but this failed to achieve accuracy above 60\%.

Our CNN reaches comparable results; however, the envisioned performance boost is not noticeable, because the RNN sequences are probably too short (5 nodes), and the sequential processing is not the bottleneck in our case.
On the other hand, the CNN also has about twice as many parameters compared to the GRU.

Overall, our best model (chosen based on validation accuracy) is RNN GRU (ours) and the best checkpoint can be downloaded from PolyBox\footnote{\url{https://polybox.ethz.ch/index.php/s/yUr8Iga0OLZ8p6B}}.

\begin{table}[btp]\centering
\begin{tabular}{lcc}
\toprule
Model    	& Validation accuracy  	 		  & Test accuracy  					\\
\midrule
RNN GRU \citep{Roemmele2017AnTest} Rand-6 (orig.)    	& 0.645  	 		  & 0.632  \\
RNN GRU \citep{Roemmele2017AnTest} Rand-3 + Back-1
+ Near-1 + LM-1 (orig.)    	& 0.656  	 		  & 0.672  \\
\midrule

RNN GRU \citep{Roemmele2017AnTest} (ours)    	& \textbf{0.703}  	 		  & 0.675  \\
RNN GRU with constant negative embedding    	& 0.682  	 		  & 0.674 \\
RNN LSTM		& 0.697 		 	  & \textbf{0.697} \\
RNN Vanilla ($\tanh$)		& 0.597 		 	  & 0.568  \\
Bi-RNN GRU		& 0.684   			  & 0.678  \\
RNN GRU with Bahdanau \citep{Bahdanau2016End-to-EndRecognition} Attention     	& 0.697	 	 		  & 0.657  \\
RNN GRU with Luong \citep{Luong2015EffectiveTranslation} Attention    	& 0.700	 	 		  & 0.675  \\
\midrule
Temporal CNN   	& 0.689	 	 		  & 0.661\\\bottomrule
\end{tabular}
\caption{Accuracy scores achieved by different models.}\label{tab:results}
\end{table}

\section{Future Work}\label{sec:futurework}
We did not attempt to use generative methods for this task, because of the discrepancy in goals between training
to generate positive endings and later using the model to generate or evaluate negative endings \citep{TCGAN} \textbf{TODO + compare in results table}.
Future work might include designing a generative model conditioned on the labels with negative ending sampling.
We liked the idea of using reinforcement learning as suggested by Fedus\,et.\,al.\,\citep{Fedus2018MaskGAN:_______},
especially because the generator in this scenario is not differentiable due to the $\mathrm{argmax}$ function for obtaining predictions.

We believe that the key to getting closer to solving the Story Cloze task is semi-supervised learning, together with generative methods. The lack of negative endings makes change this task for a simple binary classification problem to a conceptually hard problem to model. As mentioned above, training a conditional generative model (possibly a GAN) to sample realistic positive and negative endings from might prove to be useful as pre-training, after which the model could be left to improve without the label conditioning. In our opinion, several important parts in this concept are still missing.

For a more short-term approach, we would consider various semi-supervised approaches leveraging the well-trained embeddings space of the pre-trained Skip-Thought embeddings, similar to our constant negative embedding experiment. One can imagine a smart strategy of negative ending re-sampling based on the current trained model could converge to better results.

\section{Conclusion}\label{sec:conclusion}
We re-implemented the model from Roemmele\,et\,al.\,\citep{Roemmele2017AnTest} and achieved an even higher accuracy. Furthermore, we explored several variations of the model and analyzed their performance. Our experiments confirm that a GRU RNN in combination with pre-trained Skip-Thought embeddings achieves the best results. Even though our reported results are close to state-of-the-art, current models clearly aren't powerful enough with the currently available data and computation to learn a ``world-view'' notion, and therefore fall short of human performance.

\bibliography{Mendeley}
\end{document}

